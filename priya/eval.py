# -*- coding: utf-8 -*-
"""eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z7OgCgOp6XOpfEhdUmMT37XAt-UgUO8u
"""

import json
from typing import List, Optional, Dict

import openai
import evaluate
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer

class Evaluation:
    def __init__(
        self,
        dialogues: List[str],
        notes: List[str],
        soap: List[str],
        openai_api_key: Optional[str] = None
    ):
        """
        dialogues: the patient-doctor transcripts
        notes: your model-generated 'notes' (if separate from 'soap')
        soap: your model-generated SOAP notes (S/O/A/P all together)
        ref_dataset: dict with keys "dialogue", "notes", "soap" giving the gold references
        """
        self.dialogues = dialogues
        self.notes = notes
        self.soap = soap

        if openai_api_key is not None:
            openai.api_key = openai_api_key

        # for few-shot example embedding if desired
        self.sbert = SentenceTransformer("all-mpnet-base-v2")

    def evaluate_llm(
        self,
        model: str = "gpt-4",
        few_shot_examples: Optional[List[Dict[str,str]]] = None,
        criteria: List[str] = [
            "Relevance",
            "Coherence",
            "Consistency",
            "Fluency",
            "Coverage"
        ]
    ) -> List[Dict[str, float]]:
        """
        Runs an LLM-as-judge over each generated SOAP note.

        few_shot_examples: optional list of dicts
            [{"dialogue": "...", "soap": "...", "scores": {"Relevance":4, ...}}, ...]
        Returns a list of dicts, one per example, with the criteria scores.
        """
        results = []
        # build the system prompt
        system_msg = (
            "You are a clinical note quality evaluator. "
            "For the given patient-doctor transcript and generated SOAP note, "
            "rate each criterion on a scale from 1 (worst) to 5 (best):\n"
            + "\n".join(f"- {c}" for c in criteria)
            + "\nRespond ONLY with a JSON object, e.g. {"
            + ", ".join(f"\"{c}\": <num>" for c in criteria)
            + "}."
        )

        # function to build the user prompt for one example
        def build_prompt(dialogue, gen_soap):
            prompt = f"Transcript:\n{dialogue}\n\nGenerated SOAP:\n{gen_soap}"
            return prompt

        # Optionally assemble few-shot prefix
        prefix_messages = [{"role":"system","content":system_msg}]
        if few_shot_examples:
            for ex in few_shot_examples:
                user = f"Transcript:\n{ex['dialogue']}\n\nGenerated SOAP:\n{ex['soap']}"
                assistant = json.dumps(ex["scores"])
                prefix_messages += [
                    {"role":"user","content":user},
                    {"role":"assistant","content":assistant}
                ]

        # loop through your test set
        for dialogue, gen_soap in zip(self.dialogues, self.soap):
            msgs = prefix_messages + [
                {"role":"user","content": build_prompt(dialogue, gen_soap)}
            ]
            resp = openai.ChatCompletion.create(
                model=model,
                messages=msgs,
                temperature=0.0
            )
            content = resp.choices[0].message.content.strip()
            # parse JSON out
            parsed = json.loads(content)
            results.append(parsed)

        return results